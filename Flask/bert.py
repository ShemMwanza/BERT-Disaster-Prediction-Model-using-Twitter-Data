# -*- coding: utf-8 -*-
"""BERT_Disaster_Prediction_Model_using_Twitter_Data.ipynb

Automatically generated by Colaboratory.


"""

import torch 
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch import nn
from torch.optim import AdamW
from tqdm.notebook import tqdm
from torchvision import transforms
import numpy as np
import pandas as pd
from torch.optim import lr_scheduler

from transformers import BertTokenizer, BertModel

#seed
np.random.seed(112)
# device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
use_cuda = torch.cuda.is_available()
print(device)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

bert = BertModel.from_pretrained('bert-base-uncased')

"""# Dataset Class"""

class Dataset(torch.utils.data.Dataset):

  def __init__(self, df, tokenizer, istrain=True):
    self.tokenizer = tokenizer
    self.istrain = istrain
    if self.istrain:
        self.text = df['text'].values
        self.labels = df['target'].values
    else:
        self.text = df

  def __len__(self):
    if self.istrain:
        return len(self.labels)
    else:
        return len(self.text)


  def __getitem__(self, index):
    text = self.text[index]
    if self.istrain:
        labels = self.labels[index]

    inputs = self.tokenizer.encode_plus(text, max_length=25, padding='max_length', truncation=True)
    input_ids = inputs["input_ids"]
    mask = inputs["attention_mask"]
    
    if self.istrain:
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long)
        }
    else:
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
        }


"""# Prediction Function

"""

#Model prediction function
def predict(model, data, batch_size):

    dataset = Dataset(data, tokenizer=tokenizer, istrain=False)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size = batch_size,
    )
    y_pred = []
    use_cuda = torch.cuda.is_available()

    if use_cuda:

        model = model.cuda()

    with torch.no_grad():

        for i, data in tqdm(enumerate(dataloader), total= len(dataloader)):
              mask = data["mask"].to(device, dtype=torch.long)
              input_ids = data["input_ids"].to(device, dtype=torch.long)

              output = model(input_ids, mask)

              prob_max, predicted = torch.max(output, 1)

              predicted = predicted.detach().cpu().numpy()
              y_pred.extend(predicted)
    #concatenating all batch predictions
    return np.concatenate([y_pred])